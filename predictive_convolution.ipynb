{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Convolution\n",
    "\n",
    "In the spirit of Tom Erbe's SoundHack, this project is an attempt at recreating the great convolution tool that has inspired so many composers, producers, makers, and sound artists using a modern approach: Machine Learning. The result of this exercise is less dependent on an individual machine learning model, but explores the relationship between the model and the data used to train them and leverage their predictive modelings.\n",
    "\n",
    "Audio files were obtained via http://soundbible.com/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and FFT analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile as wav\n",
    "from scipy.fftpack import fft, ifft\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "fileA = './input_audio/baby-music-box_daniel-simion.wav'\n",
    "fileB = './input_audio/Warbling_Vireo-Mike_Koenig-89869915.wav'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are methods to allow for wav data to be extracted, and then data is generated from the raw waveform and using a Fast Fourier Transform (FFT) and methods to convert the resulting complex number to `phase` and `magnitude`.\n",
    "\n",
    "Note that any potential header or footer added to the wav file is not taken into account here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_check(df):\n",
    "    samples, channels = df.shape\n",
    "    if channels > 1:\n",
    "        df.drop(1, axis=1, inplace=True)\n",
    "\n",
    "def extract_wav_data(filename):\n",
    "    rate, data = wav.read(filename)\n",
    "    sin_data = np.sin(data)\n",
    "    sineRaw = pd.DataFrame(sin_data)\n",
    "    inputRaw = pd.DataFrame(data)\n",
    "    \n",
    "    drop_check(sineRaw)\n",
    "    drop_check(inputRaw)\n",
    "\n",
    "    bufferdata = pd.concat([inputRaw, sineRaw], axis=1)\n",
    "    bufferdata.columns=['raw', 'sine']\n",
    "    \n",
    "    bufferdata_raw_fft = fft_wav_data(bufferdata['raw'], 'raw')\n",
    "    bufferdata_sine_fft = fft_wav_data(bufferdata['sine'], 'sine')\n",
    "    \n",
    "    bufferdata = pd.concat([bufferdata, bufferdata_raw_fft, bufferdata_sine_fft], axis=1)\n",
    "    return bufferdata\n",
    "\n",
    "def fft_wav_data(bufferdataCol, designatorStr):\n",
    "    fftout = pd.DataFrame()\n",
    "    complexColName = '{}_complex'.format(designatorStr)\n",
    "    fftout[complexColName] = fft(bufferdataCol)\n",
    "    fftout['{}_complex_real'.format(designatorStr)] = fftout[complexColName].real\n",
    "    fftout['{}_complex_imag'.format(designatorStr)] = fftout[complexColName].imag\n",
    "    fftout['{}_magnitude'.format(designatorStr)] = np.hypot(fftout[complexColName].real, fftout[complexColName].imag)\n",
    "    fftout['{}_phase'.format(designatorStr)] = np.angle(fftout[complexColName])\n",
    "    return fftout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data returned is in a multi-dimensional array consisting of two items per sample due to the stereophonic nature of the data. At the moment, we strip out the second channel so that only one channel is processed.\n",
    "\n",
    "Here's an example of what the extracted data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataExample = extract_wav_data(fileB)\n",
    "print(dataExample.info())\n",
    "dataExample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Attempt (Failure)\n",
    "\n",
    "This is here to show how _not_ to do it. I first attempted to predict the raw waveform using the generated data above. This produced some curious results, but the output is always reflected horizantally around the midpoint of the output file's duration. As a result, I decided to try a different approach.\n",
    "\n",
    "_Note:_ The beginning and ending of the output files have a considerable spike compared to the rest of the file. I recommend deleting these pieces and then normalizing the file for more effective representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_conv_fail(wav1, wav2, model, outputfile):\n",
    "    bufferdata_A = extract_wav_data(wav1)\n",
    "    bufferdata_B = extract_wav_data(wav2)\n",
    "    X = bufferdata_A[['raw_phase', 'raw_magnitude', 'sine_phase', 'sine_magnitude', 'sine']]\n",
    "    Y = bufferdata_A['raw']\n",
    "\n",
    "    model.fit(X, Y)\n",
    "    Y_predict = model.predict(bufferdata_B[['raw_phase', 'raw_magnitude', 'sine_phase', 'sine_magnitude', 'sine']])\n",
    "\n",
    "    rate, data = wav.read(wav2)\n",
    "    scaled = (Y_predict / Y_predict.max()) * 0.95\n",
    "    wav.write(outputfile, rate, scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "bayesR = BayesianRidge(compute_score=True)\n",
    "\n",
    "pred_conv_fail(fileA, fileB, bayesR, './output_audio/fail1.wav')\n",
    "pred_conv_fail(fileB, fileA, bayesR, './output_audio/fail2.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lessons Learned\n",
    "\n",
    "So there's a couple of problems here - the data coming back seems to be 'reflected' horizontally around the midpoint of the file duration. This appears to be an artifact from the Bayesian Ridge model; however, there's still the problem of the numbers falling off to a particular level midway through the dataset, so it may be more useful to find a way to convert back to a raw waveform using an Inverse Fast-Fourier Transform (iFFT).\n",
    "\n",
    "The second is that after generating the `.hypot` and the `.angle`, we can't convert that back into the complex number needed for the ifft. So we need to anticipate the imaginary and real postions instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Successful Predictive Convolution\n",
    "\n",
    "Because of this behavior, I'm assuming that the data needs resynthesized using an inverse FFT, so rather than extracting the predicted raw dataform given a series of magnitudes and phase, I'm going to solve and predict twice:\n",
    "1. First, solve for the real number,\n",
    "2. Next, solve for the imaginary number\n",
    "\n",
    "Then the output is run through an ifft to create a 'predicted' audio file. Note that the output of the ifft is a list of complex numbers with extremely small imaginary portions. For the purposes of this demonstration, I'm only using the real portions to create the final raw waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_conv(wav1, wav2, model, outputfile):\n",
    "    bufferdata_A = extract_wav_data(wav1)\n",
    "    bufferdata_B = extract_wav_data(wav2)\n",
    "    \n",
    "    features = ['raw', 'raw_magnitude', 'raw_phase', 'sine_magnitude', 'sine_phase', 'sine']\n",
    "    \n",
    "    X_train = bufferdata_A[features]\n",
    "    Y_real = bufferdata_A['raw_complex_real']\n",
    "    Y_imag = bufferdata_A['raw_complex_imag']\n",
    "\n",
    "    # First, let's predict the real part of the complex number\n",
    "    model.fit(X_train, Y_real)\n",
    "    Y_predict_real = model.predict(bufferdata_B[features])\n",
    "\n",
    "    # Next, let's predict the imaginary part of the complex number\n",
    "    model.fit(X_train, Y_imag)\n",
    "    Y_predict_imag = model.predict(bufferdata_B[features])\n",
    "    \n",
    "    # Now let's generate complex numbers using the two and output the predicted waveform using ifft.\n",
    "    output_df = pd.concat([pd.DataFrame(Y_predict_real), pd.DataFrame(Y_predict_imag)], axis=1)\n",
    "    output_df.columns=['real', 'imag']\n",
    "    # We need to format the real and imaginary portions as a single complex number\n",
    "    output_df['complex'] = output_df['real'] + (output_df['imag'] * 1j)\n",
    "    # And strip out the imaginary portions to create the 'raw' waveform\n",
    "    output_df['raw'] = ifft(output_df['complex']).real\n",
    "    \n",
    "    rate, data = wav.read(wav2)\n",
    "    scaled = (output_df['raw'] / np.abs(output_df['raw']).max()) * 0.95\n",
    "    wav.write(outputfile, rate, scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this method defined, we can now create a couple of models and play around with the audio files. Note that each type of model produces very different results.\n",
    "\n",
    "As mentioned above, the files required some trimming and normalization to be fully appreciated. I also recommend performing some noise filtering on some of the noisier outputs. The LinearRegression model seems to create an awful lot of noise, but if noise reduction is used in software like Audacity, the output can be very interesting.\n",
    "\n",
    "Also keep in mind that these take a while to run. Raw audio has a lot of data points, and machine learning, while rapidly improving, is also still relatively slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "bayesR = BayesianRidge(compute_score=True)\n",
    "\n",
    "pred_conv(fileA, fileB, bayesR, './output_audio/bayes1.wav')\n",
    "pred_conv(fileB, fileA, bayesR, './output_audio/bayes2.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "pred_conv(fileA, fileB, lin_reg, './output_audio/lin_reg1.wav')\n",
    "pred_conv(fileB, fileA, lin_reg, './output_audio/lin_reg2.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoLars\n",
    "\n",
    "lasso = LassoLars(alpha=0.42, fit_intercept=False, eps=0.2, max_iter=200)\n",
    "\n",
    "pred_conv(fileA, fileB, lasso, './output_audio/lasso1.wav')\n",
    "pred_conv(fileB, fileA, lasso, './output_audio/lasso2.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "\n",
    "pa_reg = PassiveAggressiveRegressor(C=0.42, random_state=42, max_iter=580)\n",
    "\n",
    "pred_conv(fileA, fileB, pa_reg, './output_audio/pa_reg1.wav')\n",
    "pred_conv(fileB, fileA, pa_reg, './output_audio/pa_reg2.wav')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
